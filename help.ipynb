{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit, LeaveOneGroupOut\n",
    "import gc\n",
    "from fastai import *\n",
    "from fastai.tabular import *\n",
    "from pytorch_toolbelt import losses as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/shanmugam/fastai/ion/CAX_LogFacies_Train_File.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/shanmugam/fastai/ion/data-without-drift/train_clean.csv\")\n",
    "test = pd.read_csv(\"/home/shanmugam/fastai/ion/data-without-drift/test_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.read_csv(\"/home/shanmugam/fastai/ion/sample_submission.csv\", dtype={'time':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['filter'] = 0\n",
    "test['filter'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = pd.concat([train, test], axis=0, sort=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1['time2'] = pd.cut(ts1['time'], bins=np.linspace(0.0000, 700., num=14 + 1), labels=list(range(14)), include_lowest=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(321)\n",
    "ts1['group'] = pd.cut(ts1['time'], bins=np.linspace(0.0000, 700., num=14*125 + 1), labels=list(range(14*125)), include_lowest=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(321)\n",
    "\n",
    "y = ts1.loc[ts1['filter']==0, 'open_channels']\n",
    "group = ts1.loc[ts1['filter']==0, 'group']\n",
    "X = ts1.loc[ts1['filter']==0, 'signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(321)\n",
    "skf = GroupKFold(n_splits=5)\n",
    "splits = [x for x in skf.split(X, y, group)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = [col for col in ts1.columns if col not in ['index','filter','group', 'open_channels', 'time', 'time2']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in use_cols:\n",
    "    col_mean = ts1[col].mean()\n",
    "    ts1[col] = ts1[col].fillna(col_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds_all = np.zeros((ts1[ts1['filter']==0].shape[0], 11))\n",
    "test_preds_all = np.zeros((ts1[ts1['filter']==2].shape[0], 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ts1.loc[ts1['filter']==0, 'time']\n",
    "groups = ts1.loc[ts1['filter']==0, 'group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_splits = []\n",
    "for sp in splits:\n",
    "    new_split = []\n",
    "    new_split.append(np.unique(groups[sp[0]]))\n",
    "    new_split.append(np.unique(groups[sp[1]]))\n",
    "    new_splits.append(new_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group\n",
       "0       [[-2.76], [-2.8557], [-2.4074], [-3.1404], [-3...\n",
       "1       [[-2.7315], [-2.966], [-2.905], [-2.713], [-2....\n",
       "2       [[-2.7916], [-2.7168], [-2.9530000000000003], ...\n",
       "3       [[-2.6076], [-2.7435], [-2.798], [-3.069], [-2...\n",
       "4       [[-2.8839], [-2.9509], [-3.0634], [-3.188], [-...\n",
       "                              ...                        \n",
       "1245    [[3.9097650003934232], [3.5378961686331207], [...\n",
       "1246    [[3.3394226830619003], [4.49855394039967], [5....\n",
       "1247    [[5.48969729903853], [6.9973286257316065], [5....\n",
       "1248    [[4.369009785352983], [5.399041161614716], [5....\n",
       "1249    [[1.3412809287908494], [3.3961123348033384], [...\n",
       "Length: 1250, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts1[ts1['filter']==0].groupby('group').apply(lambda x: x[use_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval = np.array(list(ts1[ts1['filter']==0].groupby('group').apply(lambda x: x[use_cols].values)))\n",
    "test = np.array(list(ts1[ts1['filter']==2].groupby('group').apply(lambda x: x[use_cols].values)))\n",
    "trainval_y = np.array(list(ts1[ts1['filter']==0].groupby('group').apply(lambda x: x[['open_channels']].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 4000, 1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose to B x C x L\n",
    "trainval = trainval.transpose((0,2,1))\n",
    "test = test.transpose((0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_y = trainval_y.reshape(trainval_y.shape[:2])\n",
    "test_y = np.zeros((test.shape[0], trainval_y.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.tabular import *\n",
    "?FloatList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n",
    "        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n",
    "        self.counter, self.best_score = 0, None\n",
    "        self.is_maximize = is_maximize\n",
    "\n",
    "    def load_best_weights(self, model):\n",
    "        model.load_state_dict(torch.load(self.checkpoint_path))\n",
    "\n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None or \\\n",
    "        (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n",
    "            torch.save(model.state_dict(), self.checkpoint_path)\n",
    "            self.best_score, self.counter = score, 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    \n",
    "class Seq2SeqRnn(nn.Module):\n",
    "    def __init__(self, input_size, seq_len, hidden_size, output_size, num_layers=1, bidirectional=False, dropout=.3,\n",
    "            hidden_layers = [100, 200]):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        self.bidirectional=bidirectional\n",
    "        self.output_size=output_size\n",
    "        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, batch_first=True,dropout=0.5)\n",
    "#         self.relu = [nn.ReLU(inplace=True)]\n",
    "        self.bn = BatchNorm1dFlat(hidden_size*2)\n",
    "        \n",
    "         # Input Layer\n",
    "        if hidden_layers and len(hidden_layers):\n",
    "            \n",
    "#             first_layer  = nn.Linear(hidden_size*2 if bidirectional else hidden_size, hidden_layers[0])\n",
    "            first_layer  = nn.Linear(hidden_size*2 if bidirectional else hidden_size, hidden_layers[0])\n",
    "\n",
    "            # Hidden Layers\n",
    "            for i in range(len(hidden_layers) - 1):\n",
    "                self.hidden_layers = nn.ModuleList(\n",
    "                    [first_layer]+ [nn.ReLU] + [BatchNorm1dFlat(hidden_layers[i])] + [nn.Linear(hidden_layers[i], hidden_layers[i+1])] \n",
    "                )\n",
    "#             self.hidden_layers = nn.ModuleList(\n",
    "#                 [first_layer]+[nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers) - 1)]\n",
    "#             )\n",
    "            nn.init.kaiming_normal_(self.hidden_layers[0].weight.data)   \n",
    "            nn.init.kaiming_normal_(self.hidden_layers[3].weight.data)  \n",
    "#             for layer in self.hidden_layers: nn.init.kaiming_normal_(layer.weight.data) \n",
    "#             self.intermediate_layer = nn.Linear(hidden_layers[-1], self.input_size)\n",
    "            self.intermediate_layer = nn.ModuleList([nn.ReLU] + \n",
    "                [BatchNorm1dFlat(hidden_layers[-1])] + [nn.Linear(hidden_layers[-1], self.input_size)])\n",
    "#             output layers\n",
    "            self.output_layer = nn.ModuleList([nn.ReLU] + [BatchNorm1dFlat(hidden_layers[-1])] + [nn.Linear(hidden_layers[-1], output_size)])\n",
    "            \n",
    "            nn.init.kaiming_normal_(self.output_layer[2].weight.data) \n",
    "#             self.output_layer = nn.Linear(hidden_layers[-1], output_size)\n",
    "#             nn.init.kaiming_normal_(self.output_layer.weight.data) \n",
    "        else:\n",
    "            self.hidden_layers = []\n",
    "            self.intermediate_layer = nn.Linear(hidden_size*2 if bidirectional else hidden_siz, self.input_size)\n",
    "            self.output_layer = nn.Linear(hidden_size*2 if bidirectional else hidden_size, output_size)\n",
    "            nn.init.kaiming_normal_(self.output_layer.weight.data) \n",
    "\n",
    "        self.activation_fn = torch.relu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        pdb.set_trace()\n",
    "        x = x.permute(0,2,1)\n",
    "        outputs, hidden = self.rnn(x)        \n",
    "        \n",
    "        x = self.dropout(self.bn(self.activation_fn(outputs)))\n",
    "#         x = self.dropout(self.activation_fn(outputs))\n",
    "#         x = self.dropout(self.bn(outputs))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "#             x = self.activation_fn(hidden_layer(x))\n",
    "#             x = nn.BatchNorm1d(x)\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class IonDataset(Dataset):\n",
    "    \"\"\"Car dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, labels, training=True, transform=None, flip=0.5, noise_level=0, class_split=0.0):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.training = training\n",
    "        self.flip = flip\n",
    "        self.noise_level = noise_level\n",
    "        self.class_split = class_split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        data = self.data[idx]\n",
    "        labels = self.labels[idx]\n",
    "        if np.random.rand() < self.class_split:\n",
    "            data, labels = class_split(data, labels)\n",
    "        if  np.random.rand() < self.noise_level:\n",
    "            data = data * torch.FloatTensor(10000).uniform_(1-self.noise_level, 1+self.noise_level)\n",
    "        if np.random.rand() < self.flip:\n",
    "            data = torch.flip(data, dims=[1])\n",
    "            labels = np.flip(labels, axis=0).copy().astype(int)\n",
    "\n",
    "        return [data, labels.astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval = torch.Tensor(trainval)\n",
    "test = torch.Tensor(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "for index, (train_index, val_index ) in enumerate(new_splits[0:], start=0):\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "torch.nn.modules.activation.ReLU is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-f3956ce7ddb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m model=Seq2SeqRnn(input_size=trainval.shape[1], seq_len=4000, hidden_size=128, output_size=11, num_layers=2, hidden_layers=[64,64,64],\n\u001b[0;32m---> 14\u001b[0;31m                          bidirectional=True).to(device)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mtab_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataBunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtab_db\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-59728071ae4b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, seq_len, hidden_size, output_size, num_layers, bidirectional, dropout, hidden_layers)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 self.hidden_layers = nn.ModuleList(\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0;34m[\u001b[0m\u001b[0mfirst_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBatchNorm1dFlat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 )\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#             self.hidden_layers = nn.ModuleList(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, modules)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mself\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_abs_string_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36m__iadd__\u001b[0;34m(self, modules)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iadd__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mextend\u001b[0;34m(self, modules)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36madd_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             raise TypeError(\"{} is not a Module subclass\".format(\n\u001b[0;32m--> 188\u001b[0;31m                 torch.typename(module)))\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             raise TypeError(\"module name should be a string. Got {}\".format(\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.nn.modules.activation.ReLU is not a Module subclass"
     ]
    }
   ],
   "source": [
    "batchsize = 16\n",
    "train_dataset = IonDataset(trainval[train_index],  trainval_y[train_index], flip=False, noise_level=0.0, class_split=0.0)\n",
    "train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "valid_dataset = IonDataset(trainval[val_index],  trainval_y[val_index], flip=False)\n",
    "valid_dataloader = DataLoader(valid_dataset, batchsize, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "test_dataset = IonDataset(test,  test_y, flip=False, noise_level=0.0, class_split=0.0)\n",
    "test_dataloader = DataLoader(test_dataset, batchsize, shuffle=False, num_workers=8, pin_memory=True)\n",
    "test_preds_iter = np.zeros((2000000, 11))\n",
    "it = 0\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model=Seq2SeqRnn(input_size=trainval.shape[1], seq_len=4000, hidden_size=128, output_size=11, num_layers=2, hidden_layers=[64,64,64],\n",
    "                         bidirectional=True).to(device)\n",
    "tab_db = DataBunch(train_dataloader,valid_dataloader,test_dl=test_dataloader)\n",
    "learn = Learner(tab_db,model,metrics=accuracy)\n",
    "learn.loss_func = L.FocalLoss()\n",
    "# pdb.set_trace()\n",
    "learn.fit_one_cycle(5,max_lr=1e-03)\n",
    "# learn.lr_find()\n",
    "# learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [1/2 01:08<01:08]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>38.820774</td>\n",
       "      <td>#na#</td>\n",
       "      <td>01:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='36' class='' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      57.14% [36/63 00:39<00:29 37.9144]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      60.00% [3/5 03:41<02:27]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>38.832249</td>\n",
       "      <td>35.326225</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>38.334431</td>\n",
       "      <td>32.964149</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>37.927860</td>\n",
       "      <td>31.488235</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='31' class='' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      49.21% [31/63 00:34<00:35 37.7980]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-63bee29f9b62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-03\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#4   0.168433        0.088413        0.952356        00:37\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     21\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_fns_registered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(5,max_lr=1e-03,wd=0.2) #4 \t0.168433 \t0.088413 \t0.952356 \t00:37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>37.084267</td>\n",
       "      <td>22.830458</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>37.082474</td>\n",
       "      <td>22.637161</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>37.079102</td>\n",
       "      <td>22.398207</td>\n",
       "      <td>0.258574</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>37.077084</td>\n",
       "      <td>22.237785</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>37.076225</td>\n",
       "      <td>22.202454</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5,max_lr=1e-03,wd=0.2) #4 \t0.105642 \t0.062846 \t0.959638 \t00:37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.258593</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(10,max_lr=2,wd=0.2) #9 \t0.077487 \t0.052689 \t0.965836 \t00:37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.077504</td>\n",
       "      <td>0.052646</td>\n",
       "      <td>0.965893</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.075643</td>\n",
       "      <td>0.052545</td>\n",
       "      <td>0.965881</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.075885</td>\n",
       "      <td>0.052111</td>\n",
       "      <td>0.966223</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.074461</td>\n",
       "      <td>0.052644</td>\n",
       "      <td>0.965718</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.072923</td>\n",
       "      <td>0.051417</td>\n",
       "      <td>0.965740</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.071123</td>\n",
       "      <td>0.050809</td>\n",
       "      <td>0.965617</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.069802</td>\n",
       "      <td>0.050763</td>\n",
       "      <td>0.965054</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.068603</td>\n",
       "      <td>0.049812</td>\n",
       "      <td>0.965990</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.067997</td>\n",
       "      <td>0.049176</td>\n",
       "      <td>0.966812</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.066466</td>\n",
       "      <td>0.049494</td>\n",
       "      <td>0.966660</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.065956</td>\n",
       "      <td>0.048839</td>\n",
       "      <td>0.966634</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.065846</td>\n",
       "      <td>0.048410</td>\n",
       "      <td>0.966815</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.064324</td>\n",
       "      <td>0.048488</td>\n",
       "      <td>0.967063</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.063341</td>\n",
       "      <td>0.048402</td>\n",
       "      <td>0.967246</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.063126</td>\n",
       "      <td>0.048121</td>\n",
       "      <td>0.967307</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.063197</td>\n",
       "      <td>0.048225</td>\n",
       "      <td>0.967228</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.062829</td>\n",
       "      <td>0.048054</td>\n",
       "      <td>0.967132</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.063233</td>\n",
       "      <td>0.047953</td>\n",
       "      <td>0.967233</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.064104</td>\n",
       "      <td>0.047969</td>\n",
       "      <td>0.967235</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.064144</td>\n",
       "      <td>0.047946</td>\n",
       "      <td>0.967258</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(20,max_lr=1e-03,wd=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.058972</td>\n",
       "      <td>0.047453</td>\n",
       "      <td>0.967437</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.060869</td>\n",
       "      <td>0.047358</td>\n",
       "      <td>0.967298</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.059651</td>\n",
       "      <td>0.047538</td>\n",
       "      <td>0.966973</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.060599</td>\n",
       "      <td>0.048216</td>\n",
       "      <td>0.966734</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.062677</td>\n",
       "      <td>0.047507</td>\n",
       "      <td>0.966986</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.108158</td>\n",
       "      <td>0.054906</td>\n",
       "      <td>0.964487</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.077356</td>\n",
       "      <td>0.048419</td>\n",
       "      <td>0.966872</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.066978</td>\n",
       "      <td>0.047832</td>\n",
       "      <td>0.966939</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.062759</td>\n",
       "      <td>0.047731</td>\n",
       "      <td>0.967206</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.062792</td>\n",
       "      <td>0.047389</td>\n",
       "      <td>0.967272</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.062583</td>\n",
       "      <td>0.047276</td>\n",
       "      <td>0.967092</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.061945</td>\n",
       "      <td>0.047187</td>\n",
       "      <td>0.967170</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.061269</td>\n",
       "      <td>0.047070</td>\n",
       "      <td>0.967303</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.060358</td>\n",
       "      <td>0.046872</td>\n",
       "      <td>0.967338</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.059288</td>\n",
       "      <td>0.046926</td>\n",
       "      <td>0.967264</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.060260</td>\n",
       "      <td>0.046961</td>\n",
       "      <td>0.967285</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.060138</td>\n",
       "      <td>0.046846</td>\n",
       "      <td>0.967390</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.059059</td>\n",
       "      <td>0.046679</td>\n",
       "      <td>0.967435</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.059393</td>\n",
       "      <td>0.046706</td>\n",
       "      <td>0.967465</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.058376</td>\n",
       "      <td>0.046760</td>\n",
       "      <td>0.967414</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5,max_lr=1e-04,wd=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.060139</td>\n",
       "      <td>0.046635</td>\n",
       "      <td>0.967425</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.059982</td>\n",
       "      <td>0.046715</td>\n",
       "      <td>0.967444</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.059452</td>\n",
       "      <td>0.046654</td>\n",
       "      <td>0.967440</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.058777</td>\n",
       "      <td>0.046662</td>\n",
       "      <td>0.967441</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.058645</td>\n",
       "      <td>0.046662</td>\n",
       "      <td>0.967451</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5,max_lr=1e-05,wd=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.059947</td>\n",
       "      <td>0.047304</td>\n",
       "      <td>0.966473</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.060153</td>\n",
       "      <td>0.046842</td>\n",
       "      <td>0.967079</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.058917</td>\n",
       "      <td>0.046621</td>\n",
       "      <td>0.967440</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.058592</td>\n",
       "      <td>0.046483</td>\n",
       "      <td>0.967489</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.058787</td>\n",
       "      <td>0.046536</td>\n",
       "      <td>0.967511</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5,max_lr=1e-03/2,wd=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('model938')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_db = DataBunch(train_dataloader,valid_dataloader,test_dl=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=DataBunch;\n",
       "\n",
       "Train: <__main__.IonDataset object at 0x7fa4d4d42890>;\n",
       "\n",
       "Valid: <__main__.IonDataset object at 0x7fa4d4e89910>;\n",
       "\n",
       "Test: <__main__.IonDataset object at 0x7fa4b38fe290>, model=Seq2SeqRnn(\n",
       "  (rnn): GRU(1, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (bn): BatchNorm1dFlat(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (intermediate_layer): ModuleList(\n",
       "    (0): ReLU(inplace=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (output_layer): ModuleList(\n",
       "    (0): ReLU(inplace=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Linear(in_features=64, out_features=11, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FocalLoss(), metrics=[<function accuracy at 0x7fa53626d320>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): GRU(1, 128, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (1): BatchNorm1dFlat(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): Linear(in_features=64, out_features=11, bias=True)\n",
       "  (12): Dropout(p=0.3, inplace=False)\n",
       ")], add_time=True, silent=False, cb_fns_registered=True)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "            pred_list = []\n",
    "            for x, y in test_dataloader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                predictions = model(x[:,:trainval.shape[1],:])\n",
    "                predictions_ = predictions.view(-1, predictions.shape[-1]) \n",
    "\n",
    "                pred_list.append(F.softmax(predictions_, dim=1).cpu().numpy())\n",
    "            test_preds = np.vstack(pred_list)\n",
    "test_preds_iter += test_preds\n",
    "test_preds_all += test_preds\n",
    "if not os.path.exists(\"./predictions/test\"):\n",
    "    os.makedirs(\"./predictions/test\")\n",
    "np.save('./predictions/test/gru_clean_fold_{}_iter_{}_raw.npy'.format(index, it), arr=test_preds_iter)\n",
    "np.save('./predictions/test/gru_clean_fold_{}_raw.npy'.format(index), arr=test_preds_all)\n",
    "\n",
    "test_preds_all = test_preds_all/np.sum(test_preds_all, axis=1)[:, None]\n",
    "test_pred_frame = pd.DataFrame({'time': ss['time'].astype(str),\n",
    "                                'open_channels': np.argmax(test_preds_all, axis=1)})\n",
    "test_pred_frame.to_csv(\"./sub_gru.csv\", index=False)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 11)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/shanmugam/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m(32)\u001b[0;36mloss_batch\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     30 \u001b[0;31m\u001b[0;31m#    out_ = out.view(-1, out.shape[-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     31 \u001b[0;31m\u001b[0;31m#    y_ = yb[0].view(-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 32 \u001b[0;31m    \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     33 \u001b[0;31m\u001b[0;31m#    loss = loss_func(out_, y_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     34 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m/home/shanmugam/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m(61)\u001b[0;36mvalidate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     59 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     60 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 61 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     62 \u001b[0;31m            \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     63 \u001b[0;31m            \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[0;32m/home/shanmugam/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m(46)\u001b[0;36mget_preds\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     44 \u001b[0;31m    \u001b[0;34m\"Tuple of predictions and targets, and optional losses (if `loss_func`) using `dl`, max batches `n_batch`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     45 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 46 \u001b[0;31m    res = [torch.cat(o).cpu() for o in\n",
      "\u001b[0m\u001b[0;32m     47 \u001b[0;31m           zip(*validate(model, dl, cb_handler=cb_handler, pbar=pbar, average=False, n_batch=n_batch))]\n",
      "\u001b[0m\u001b[0;32m     48 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> p model\n",
      "Seq2SeqRnn(\n",
      "  (rnn): GRU(1, 64, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (intermediate_layer): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (output_layer): Linear(in_features=64, out_features=11, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n",
      "ipdb> p cb_handler\n",
      "CallbackHandler(callbacks=[], metrics=[], beta=0.98)\n",
      "ipdb> p n_batch\n",
      "None\n",
      "ipdb> l\n",
      "\u001b[1;32m     41 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     42 \u001b[0mdef get_preds(model:nn.Module, dl:DataLoader, pbar:Optional[PBar]=None, cb_handler:Optional[CallbackHandler]=None,\n",
      "\u001b[1;32m     43 \u001b[0m              activ:nn.Module=None, loss_func:OptLossFunc=None, n_batch:Optional[int]=None) -> List[Tensor]:\n",
      "\u001b[1;32m     44 \u001b[0m    \u001b[0;34m\"Tuple of predictions and targets, and optional losses (if `loss_func`) using `dl`, max batches `n_batch`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     45 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 46 \u001b[0;31m    res = [torch.cat(o).cpu() for o in\n",
      "\u001b[0m\u001b[1;32m     47 \u001b[0m           zip(*validate(model, dl, cb_handler=cb_handler, pbar=pbar, average=False, n_batch=n_batch))]\n",
      "\u001b[1;32m     48 \u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     49 \u001b[0m        \u001b[0;32mwith\u001b[0m \u001b[0mNoneReduceOnCPU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     50 \u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mactiv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     51 \u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "ipdb> p res\n",
      "*** NameError: name 'res' is not defined\n",
      "ipdb> l\n",
      "\u001b[1;32m     52 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     53 \u001b[0mdef validate(model:nn.Module, dl:DataLoader, loss_func:OptLossFunc=None, cb_handler:Optional[CallbackHandler]=None,\n",
      "\u001b[1;32m     54 \u001b[0m             pbar:Optional[PBar]=None, average=True, n_batch:Optional[int]=None)->Iterator[Tuple[Union[Tensor,int],...]]:\n",
      "\u001b[1;32m     55 \u001b[0m    \u001b[0;34m\"Calculate `loss_func` of `model` on `dl` in evaluation mode.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     56 \u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     57 \u001b[0m    \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     58 \u001b[0m        \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     59 \u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     60 \u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     61 \u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     62 \u001b[0m            \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "ipdb> p dl\n",
      "DeviceDataLoader(dl=<torch.utils.data.dataloader.DataLoader object at 0x7fc245967dd0>, device=device(type='cuda', index=0), tfms=[], collate_fn=<function data_collate at 0x7fc270a677a0>)\n",
      "ipdb> p val_loss\n",
      "*** NameError: name 'val_loss' is not defined\n",
      "ipdb> p xb\n",
      "*** NameError: name 'xb' is not defined\n",
      "ipdb> l\n",
      "\u001b[1;32m     63 \u001b[0m            \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     64 \u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     65 \u001b[0m            \u001b[0mnums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_el\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     66 \u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     67 \u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mn_batch\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>=\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     68 \u001b[0m        \u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     69 \u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     70 \u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     71 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     72 \u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLossFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     73 \u001b[0m    \u001b[0;34m\"Simple training of `model` for 1 epoch of `dl` using optim `opt` and loss function `loss_func`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipdb> p xb\n",
      "*** NameError: name 'xb' is not defined\n",
      "ipdb> p val_losses\n",
      "*** NameError: name 'val_losses' is not defined\n",
      "ipdb> l\n",
      "\u001b[1;32m     74 \u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     75 \u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     76 \u001b[0m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     77 \u001b[0m        \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     78 \u001b[0m        \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     79 \u001b[0m        \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     80 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     81 \u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     82 \u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0mBasicLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     83 \u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     84 \u001b[0m    \u001b[0mloss_func\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLossFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "ipdb> l\n",
      "\u001b[1;32m     85 \u001b[0m    \u001b[0mopt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     86 \u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mDataBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     87 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     88 \u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mBasicLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackList\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mOptMetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     89 \u001b[0m    \u001b[0;34m\"Fit the `model` on `data` and learn using `loss_func` and `opt`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     90 \u001b[0m    assert len(learn.data.train_dl) != 0, f\"\"\"Your training dataloader is empty, can't train a model.\n",
      "\u001b[1;32m     91 \u001b[0m        Use a smaller batch size (batch size={learn.data.train_dl.batch_size} for {len(learn.data.train_dl.dataset)} elements).\"\"\"\n",
      "\u001b[1;32m     92 \u001b[0m    \u001b[0mcb_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCallbackHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     93 \u001b[0m    \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaster_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     94 \u001b[0m    \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     95 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "ipdb> l\n",
      "\u001b[1;32m     96 \u001b[0m    \u001b[0mexception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     97 \u001b[0m    \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     98 \u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     99 \u001b[0m            \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    100 \u001b[0m            \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    101 \u001b[0m            \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    102 \u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    103 \u001b[0m                \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    104 \u001b[0m                \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    105 \u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    106 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "ipdb> l\n",
      "\u001b[1;32m    107 \u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_validate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    108 \u001b[0m                val_loss = validate(learn.model, learn.data.valid_dl, loss_func=learn.loss_func,\n",
      "\u001b[1;32m    109 \u001b[0m                                       cb_handler=cb_handler, pbar=pbar)\n",
      "\u001b[1;32m    110 \u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    111 \u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    112 \u001b[0m    \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    113 \u001b[0m        \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    114 \u001b[0m        \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    115 \u001b[0m    \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    116 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    117 \u001b[0mloss_func_name2activ = {'cross_entropy_loss': F.softmax, 'nll_loss': torch.exp, 'poisson_nll_loss': torch.exp,\n",
      "\n",
      "ipdb> c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/32 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-a1d5f98ae102>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'debug'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDatasetType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(self, ds_type, activ, with_loss, n_batch, pbar)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_fns_registered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         return get_preds(self.model, self.dl(ds_type), cb_handler=CallbackHandler(self.callbacks),\n\u001b[0m\u001b[1;32m    348\u001b[0m                          activ=activ, loss_func=lf, n_batch=n_batch, pbar=pbar)\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(model, dl, pbar, cb_handler, activ, loss_func, n_batch)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;34m\"Tuple of predictions and targets, and optional losses (if `loss_func`) using `dl`, max batches `n_batch`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     res = [torch.cat(o).cpu() for o in\n\u001b[0m\u001b[1;32m     47\u001b[0m            zip(*validate(model, dl, cb_handler=cb_handler, pbar=pbar, average=False, n_batch=n_batch))]\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloss_func\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, dl, loss_func, cb_handler, pbar, average, n_batch)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#    out_ = out.view(-1, out.shape[-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#    y_ = yb[0].view(-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;31m#    loss = loss_func(out_, y_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "%debug\n",
    "preds, _ = learn.get_preds(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "> <ipython-input-21-94d360abc00d>(22)<module>()\n",
      "-> learn.fit_one_cycle(1,max_lr=1e-03)\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.146713</td>\n",
       "      <td>0.719118</td>\n",
       "      <td>0.340028</td>\n",
       "      <td>00:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 1\n",
      "> <ipython-input-21-94d360abc00d>(21)<module>()\n",
      "-> pdb.set_trace()\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.153987</td>\n",
       "      <td>0.724897</td>\n",
       "      <td>0.299304</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 2\n",
      "> <ipython-input-21-94d360abc00d>(22)<module>()\n",
      "-> learn.fit_one_cycle(1,max_lr=1e-03)\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.315211</td>\n",
       "      <td>0.745958</td>\n",
       "      <td>0.424823</td>\n",
       "      <td>00:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 3\n",
      "> <ipython-input-21-94d360abc00d>(21)<module>()\n",
      "-> pdb.set_trace()\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.182423</td>\n",
       "      <td>0.651028</td>\n",
       "      <td>0.445214</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 4\n",
      "> <ipython-input-21-94d360abc00d>(22)<module>()\n",
      "-> learn.fit_one_cycle(1,max_lr=1e-03)\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.050273</td>\n",
       "      <td>0.600786</td>\n",
       "      <td>0.614630</td>\n",
       "      <td>00:37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for index, (train_index, val_index ) in enumerate(new_splits[0:], start=0):\n",
    "    print(\"Fold : {}\".format(index))\n",
    "    \n",
    "    batchsize = 16\n",
    "    train_dataset = IonDataset(trainval[train_index],  trainval_y[train_index], flip=False, noise_level=0.0, class_split=0.0)\n",
    "    train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    valid_dataset = IonDataset(trainval[val_index],  trainval_y[val_index], flip=False)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batchsize, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    test_dataset = IonDataset(test,  test_y, flip=False, noise_level=0.0, class_split=0.0)\n",
    "    test_dataloader = DataLoader(test_dataset, batchsize, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    test_preds_iter = np.zeros((2000000, 11))\n",
    "    it = 0\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model=Seq2SeqRnn(input_size=trainval.shape[1], seq_len=4000, hidden_size=64, output_size=11, num_layers=2, hidden_layers=[64,64,64],\n",
    "                         bidirectional=True).to(device)\n",
    "    tab_db = DataBunch(train_dataloader,valid_dataloader,test_dataloader)\n",
    "    learn = Learner(tab_db,model,metrics=accuracy)\n",
    "    learn.loss_func = L.FocalLoss()\n",
    "    pdb.set_trace()\n",
    "    learn.fit_one_cycle(1,max_lr=1e-03)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold : 0\n",
      "Epoch : 0\n",
      "learning_rate: 0.000001000\n",
      "torch.Size([16, 4000, 11]) torch.Size([16, 4000])\n",
      "torch.Size([64000, 11]) torch.Size([64000])\n",
      "torch.Size([16, 4000, 11]) torch.Size([16, 4000])\n",
      "torch.Size([64000, 11]) torch.Size([64000])\n",
      "torch.Size([16, 4000, 11]) torch.Size([16, 4000])\n",
      "torch.Size([64000, 11]) torch.Size([64000])\n",
      "torch.Size([16, 4000, 11]) torch.Size([16, 4000])\n",
      "torch.Size([64000, 11]) torch.Size([64000])\n",
      "torch.Size([16, 4000, 11]) torch.Size([16, 4000])\n",
      "torch.Size([64000, 11]) torch.Size([64000])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-84b842f8780c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;31m# backward pass: compute gradient of the loss with respect to model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0;31m# perform a single optimization step (parameter update)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for index, (train_index, val_index ) in enumerate(new_splits[0:], start=0):\n",
    "    print(\"Fold : {}\".format(index))\n",
    "    \n",
    "    batchsize = 16\n",
    "    train_dataset = IonDataset(trainval[train_index],  trainval_y[train_index], flip=False, noise_level=0.0, class_split=0.0)\n",
    "    train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    valid_dataset = IonDataset(trainval[val_index],  trainval_y[val_index], flip=False)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batchsize, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    test_dataset = IonDataset(test,  test_y, flip=False, noise_level=0.0, class_split=0.0)\n",
    "    test_dataloader = DataLoader(test_dataset, batchsize, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    test_preds_iter = np.zeros((2000000, 11))\n",
    "    it = 0\n",
    "    device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model=Seq2SeqRnn(input_size=trainval.shape[1], seq_len=4000, hidden_size=64, output_size=11, num_layers=2, hidden_layers=[64,64,64],\n",
    "                         bidirectional=True).to(device)\n",
    "    for it in range(1):\n",
    "        device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        model=Seq2SeqRnn(input_size=trainval.shape[1], seq_len=4000, hidden_size=64, output_size=11, num_layers=2, hidden_layers=[64,64,64],\n",
    "                         bidirectional=True).to(device)\n",
    "    \n",
    "        no_of_epochs = 1\n",
    "        early_stopping = EarlyStopping(patience=20, is_maximize=True, checkpoint_path=\"./models/gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index, it))\n",
    "        criterion = L.FocalLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        schedular = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, max_lr=0.001, epochs=no_of_epochs,\n",
    "                                                steps_per_epoch=len(train_dataloader))\n",
    "        avg_train_losses, avg_valid_losses = [], [] \n",
    "    \n",
    "    \n",
    "        for epoch in range(no_of_epochs):\n",
    "#             start_time = time.time()\n",
    "    \n",
    "            print(\"Epoch : {}\".format(epoch))\n",
    "            print( \"learning_rate: {:0.9f}\".format(schedular.get_lr()[0]))\n",
    "            train_losses, valid_losses = [], []\n",
    "    \n",
    "            model.train() # prep model for training\n",
    "            train_preds, train_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n",
    "    \n",
    "            for x, y in train_dataloader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                predictions = model(x)\n",
    "                \n",
    "                predictions_ = predictions.view(-1, predictions.shape[-1]) \n",
    "                y_ = y.view(-1)\n",
    "                print(predictions.shape,y.shape)\n",
    "                print(predictions_.shape, y_.shape)\n",
    "                loss = criterion(predictions_, y_)\n",
    "                # backward pass: compute gradient of the loss with respect to model parameters\n",
    "                loss.backward()\n",
    "                # perform a single optimization step (parameter update)\n",
    "                optimizer.step()\n",
    "                schedular.step()\n",
    "                # record training lossa\n",
    "                train_losses.append(loss.item())\n",
    "    \n",
    "                train_true = torch.cat([train_true, y_], 0)\n",
    "                train_preds = torch.cat([train_preds, predictions_], 0)\n",
    "\n",
    "            model.eval() # prep model for evaluation\n",
    "            val_preds, val_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n",
    "            with torch.no_grad():\n",
    "                for x, y in valid_dataloader:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "    \n",
    "                    predictions = model(x[:,:trainval.shape[1],:])\n",
    "                    predictions_ = predictions.view(-1, predictions.shape[-1]) \n",
    "                    y_ = y.view(-1)\n",
    "    \n",
    "                    loss = criterion(predictions_, y_)\n",
    "                    valid_losses.append(loss.item())\n",
    "        \n",
    "                    val_true = torch.cat([val_true, y_], 0)\n",
    "                    val_preds = torch.cat([val_preds, predictions_], 0)\n",
    "\n",
    "            # calculate average loss over an epoch\n",
    "            train_loss = np.average(train_losses)\n",
    "            valid_loss = np.average(valid_losses)\n",
    "            avg_train_losses.append(train_loss)\n",
    "            avg_valid_losses.append(valid_loss)\n",
    "            \n",
    "            print( \"train_loss: {:0.6f}, valid_loss: {:0.6f}\".format(train_loss, valid_loss))\n",
    "\n",
    "            train_score = f1_score(train_true.cpu().detach().numpy(), train_preds.cpu().detach().numpy().argmax(1), labels=list(range(11)), average='macro')\n",
    "    \n",
    "            val_score = f1_score(val_true.cpu().detach().numpy(), val_preds.cpu().detach().numpy().argmax(1), labels=list(range(11)), average='macro')\n",
    "            print( \"train_f1: {:0.6f}, valid_f1: {:0.6f}\".format(train_score, val_score))\n",
    "    \n",
    "            if early_stopping(val_score, model):\n",
    "                print(\"Early Stopping...\")\n",
    "                print(\"Best Val Score: {:0.6f}\".format(early_stopping.best_score))\n",
    "                break\n",
    "    \n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "        model.load_state_dict(torch.load(\"./models/gru_clean_checkpoint_fold_{}_iter_{}.pt\".format(index, it)))\n",
    "        with torch.no_grad():\n",
    "            pred_list = []\n",
    "            for x, y in test_dataloader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                predictions = model(x[:,:trainval.shape[1],:])\n",
    "                predictions_ = predictions.view(-1, predictions.shape[-1]) \n",
    "\n",
    "                pred_list.append(F.softmax(predictions_, dim=1).cpu().numpy())\n",
    "            test_preds = np.vstack(pred_list)\n",
    "       \n",
    "        test_preds_iter += test_preds\n",
    "        test_preds_all += test_preds\n",
    "        if not os.path.exists(\"./predictions/test\"):\n",
    "            os.makedirs(\"./predictions/test\")\n",
    "        np.save('./predictions/test/gru_clean_fold_{}_iter_{}_raw.npy'.format(index, it), arr=test_preds_iter)\n",
    "        np.save('./predictions/test/gru_clean_fold_{}_raw.npy'.format(index), arr=test_preds_all)\n",
    "\n",
    "# test_preds_all = test_preds_all/np.sum(test_preds_all, axis=1)[:, None]\n",
    "# test_pred_frame = pd.DataFrame({'time': ss['time'].astype(str),\n",
    "#                                 'open_channels': np.argmax(test_preds_all, axis=1)})\n",
    "# test_pred_frame.to_csv(\"./gru_preds.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc(Learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target size (torch.Size([16, 4000])) must be the same as input size (torch.Size([16, 11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L.FocalLoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.pivot_table(train, values='GR', index=['well_id'], columns=['row_id'])\n",
    "y_train = pd.pivot_table(train, values='label', index=['well_id'], columns=['row_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>row_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1090</th>\n",
       "      <th>1091</th>\n",
       "      <th>1092</th>\n",
       "      <th>1093</th>\n",
       "      <th>1094</th>\n",
       "      <th>1095</th>\n",
       "      <th>1096</th>\n",
       "      <th>1097</th>\n",
       "      <th>1098</th>\n",
       "      <th>1099</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>143.51</td>\n",
       "      <td>112.790928</td>\n",
       "      <td>123.531856</td>\n",
       "      <td>111.692784</td>\n",
       "      <td>123.613712</td>\n",
       "      <td>120.414641</td>\n",
       "      <td>123.145569</td>\n",
       "      <td>114.216497</td>\n",
       "      <td>119.387425</td>\n",
       "      <td>132.728353</td>\n",
       "      <td>...</td>\n",
       "      <td>142.881647</td>\n",
       "      <td>137.732575</td>\n",
       "      <td>145.843503</td>\n",
       "      <td>133.434431</td>\n",
       "      <td>144.205359</td>\n",
       "      <td>139.136288</td>\n",
       "      <td>158.257216</td>\n",
       "      <td>139.508144</td>\n",
       "      <td>130.589072</td>\n",
       "      <td>154.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>122.26</td>\n",
       "      <td>112.872757</td>\n",
       "      <td>120.125514</td>\n",
       "      <td>122.488271</td>\n",
       "      <td>132.111028</td>\n",
       "      <td>117.603785</td>\n",
       "      <td>117.206542</td>\n",
       "      <td>114.179299</td>\n",
       "      <td>115.182056</td>\n",
       "      <td>118.134813</td>\n",
       "      <td>...</td>\n",
       "      <td>150.295187</td>\n",
       "      <td>150.337944</td>\n",
       "      <td>133.380701</td>\n",
       "      <td>155.813458</td>\n",
       "      <td>159.656215</td>\n",
       "      <td>158.788972</td>\n",
       "      <td>139.421729</td>\n",
       "      <td>157.724486</td>\n",
       "      <td>159.127243</td>\n",
       "      <td>137.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>125.94</td>\n",
       "      <td>121.713658</td>\n",
       "      <td>112.027316</td>\n",
       "      <td>115.600974</td>\n",
       "      <td>129.104631</td>\n",
       "      <td>111.938289</td>\n",
       "      <td>132.901947</td>\n",
       "      <td>110.015605</td>\n",
       "      <td>118.769263</td>\n",
       "      <td>108.452921</td>\n",
       "      <td>...</td>\n",
       "      <td>137.417079</td>\n",
       "      <td>150.210737</td>\n",
       "      <td>152.374395</td>\n",
       "      <td>143.668053</td>\n",
       "      <td>136.331711</td>\n",
       "      <td>134.055369</td>\n",
       "      <td>148.719026</td>\n",
       "      <td>153.752684</td>\n",
       "      <td>134.256342</td>\n",
       "      <td>151.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>109.03</td>\n",
       "      <td>122.551838</td>\n",
       "      <td>122.963676</td>\n",
       "      <td>113.355514</td>\n",
       "      <td>115.347352</td>\n",
       "      <td>124.439190</td>\n",
       "      <td>118.081028</td>\n",
       "      <td>133.792866</td>\n",
       "      <td>120.034704</td>\n",
       "      <td>104.266542</td>\n",
       "      <td>...</td>\n",
       "      <td>150.073458</td>\n",
       "      <td>156.795296</td>\n",
       "      <td>149.217134</td>\n",
       "      <td>144.178972</td>\n",
       "      <td>135.520810</td>\n",
       "      <td>139.422648</td>\n",
       "      <td>156.184486</td>\n",
       "      <td>143.236324</td>\n",
       "      <td>125.958162</td>\n",
       "      <td>149.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>109.01</td>\n",
       "      <td>132.092739</td>\n",
       "      <td>111.845478</td>\n",
       "      <td>105.268217</td>\n",
       "      <td>115.480955</td>\n",
       "      <td>110.033694</td>\n",
       "      <td>130.016433</td>\n",
       "      <td>117.889172</td>\n",
       "      <td>113.621911</td>\n",
       "      <td>108.794650</td>\n",
       "      <td>...</td>\n",
       "      <td>150.875350</td>\n",
       "      <td>121.358089</td>\n",
       "      <td>139.830828</td>\n",
       "      <td>130.123567</td>\n",
       "      <td>143.416306</td>\n",
       "      <td>138.229045</td>\n",
       "      <td>121.911783</td>\n",
       "      <td>137.074522</td>\n",
       "      <td>124.117261</td>\n",
       "      <td>133.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "row_id     0           1           2           3           4           5     \\\n",
       "well_id                                                                       \n",
       "0        143.51  112.790928  123.531856  111.692784  123.613712  120.414641   \n",
       "1        122.26  112.872757  120.125514  122.488271  132.111028  117.603785   \n",
       "2        125.94  121.713658  112.027316  115.600974  129.104631  111.938289   \n",
       "3        109.03  122.551838  122.963676  113.355514  115.347352  124.439190   \n",
       "4        109.01  132.092739  111.845478  105.268217  115.480955  110.033694   \n",
       "\n",
       "row_id         6           7           8           9     ...        1090  \\\n",
       "well_id                                                  ...               \n",
       "0        123.145569  114.216497  119.387425  132.728353  ...  142.881647   \n",
       "1        117.206542  114.179299  115.182056  118.134813  ...  150.295187   \n",
       "2        132.901947  110.015605  118.769263  108.452921  ...  137.417079   \n",
       "3        118.081028  133.792866  120.034704  104.266542  ...  150.073458   \n",
       "4        130.016433  117.889172  113.621911  108.794650  ...  150.875350   \n",
       "\n",
       "row_id         1091        1092        1093        1094        1095  \\\n",
       "well_id                                                               \n",
       "0        137.732575  145.843503  133.434431  144.205359  139.136288   \n",
       "1        150.337944  133.380701  155.813458  159.656215  158.788972   \n",
       "2        150.210737  152.374395  143.668053  136.331711  134.055369   \n",
       "3        156.795296  149.217134  144.178972  135.520810  139.422648   \n",
       "4        121.358089  139.830828  130.123567  143.416306  138.229045   \n",
       "\n",
       "row_id         1096        1097        1098    1099  \n",
       "well_id                                              \n",
       "0        158.257216  139.508144  130.589072  154.93  \n",
       "1        139.421729  157.724486  159.127243  137.45  \n",
       "2        148.719026  153.752684  134.256342  151.32  \n",
       "3        156.184486  143.236324  125.958162  149.23  \n",
       "4        121.911783  137.074522  124.117261  133.20  \n",
       "\n",
       "[5 rows x 1100 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>row_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1090</th>\n",
       "      <th>1091</th>\n",
       "      <th>1092</th>\n",
       "      <th>1093</th>\n",
       "      <th>1094</th>\n",
       "      <th>1095</th>\n",
       "      <th>1096</th>\n",
       "      <th>1097</th>\n",
       "      <th>1098</th>\n",
       "      <th>1099</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "row_id   0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "well_id                                                              ...   \n",
       "0           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "1           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "2           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "3           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "4           0     0     0     0     0     0     0     0     0     0  ...   \n",
       "\n",
       "row_id   1090  1091  1092  1093  1094  1095  1096  1097  1098  1099  \n",
       "well_id                                                              \n",
       "0           0     0     0     0     0     0     0     0     0     0  \n",
       "1           0     0     0     0     0     0     0     0     0     0  \n",
       "2           0     0     0     0     0     0     0     0     0     0  \n",
       "3           0     0     0     0     0     0     0     0     0     0  \n",
       "4           0     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[5 rows x 1100 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "folds = KFold(n_splits=5, random_state=100, shuffle=True)\n",
    "indices= [(train_index, test_index) for (train_index, test_index) in folds.split(X_train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   4,    5,    8,   10,   12,   16,   25,   28,   29,   31,   44,\n",
       "         56,   57,   63,   65,   66,   68,   69,   71,   80,   93,   97,\n",
       "         98,  103,  105,  107,  110,  116,  117,  130,  134,  138,  139,\n",
       "        149,  150,  151,  153,  159,  160,  167,  174,  182,  185,  190,\n",
       "        205,  211,  213,  219,  221,  229,  231,  242,  247,  249,  258,\n",
       "        259,  263,  269,  288,  290,  291,  294,  302,  308,  309,  314,\n",
       "        316,  317,  323,  328,  332,  338,  342,  344,  349,  352,  353,\n",
       "        355,  356,  359,  362,  366,  367,  370,  385,  388,  403,  408,\n",
       "        411,  413,  416,  423,  427,  432,  434,  435,  440,  444,  452,\n",
       "        459,  463,  469,  480,  486,  492,  496,  504,  510,  519,  521,\n",
       "        525,  526,  530,  532,  534,  536,  547,  551,  552,  553,  564,\n",
       "        576,  585,  591,  593,  595,  598,  601,  605,  610,  614,  616,\n",
       "        621,  622,  629,  631,  642,  646,  647,  650,  652,  656,  669,\n",
       "        672,  676,  679,  686,  687,  688,  699,  702,  704,  716,  718,\n",
       "        719,  721,  723,  726,  730,  743,  749,  752,  766,  772,  774,\n",
       "        778,  779,  780,  781,  785,  786,  789,  790,  791,  792,  798,\n",
       "        799,  807,  828,  836,  841,  845,  852,  855,  857,  858,  859,\n",
       "        883,  886,  888,  892,  901,  908,  917,  918,  930,  934,  937,\n",
       "        945,  946,  947,  952,  957,  962,  965,  967,  970,  974,  977,\n",
       "        981,  995,  999, 1006, 1011, 1012, 1013, 1019, 1027, 1039, 1044,\n",
       "       1047, 1054, 1055, 1058, 1063, 1064, 1065, 1067, 1090, 1098, 1107,\n",
       "       1129, 1135, 1139, 1145, 1156, 1158, 1166, 1171, 1181, 1184, 1188,\n",
       "       1193, 1204, 1219, 1223, 1225, 1229, 1237, 1238, 1243, 1244, 1248,\n",
       "       1249, 1257, 1259, 1262, 1264, 1266, 1267, 1273, 1280, 1282, 1283,\n",
       "       1291, 1292, 1295, 1297, 1308, 1311, 1313, 1322, 1324, 1327, 1336,\n",
       "       1337, 1339, 1346, 1352, 1354, 1355, 1356, 1358, 1359, 1360, 1364,\n",
       "       1367, 1369, 1378, 1396, 1398, 1399, 1406, 1419, 1423, 1426, 1429,\n",
       "       1432, 1436, 1439, 1444, 1446, 1457, 1463, 1465, 1466, 1469, 1472,\n",
       "       1479, 1481, 1483, 1487, 1491, 1493, 1499, 1515, 1516, 1521, 1529,\n",
       "       1535, 1547, 1557, 1560, 1567, 1573, 1583, 1588, 1589, 1592, 1607,\n",
       "       1614, 1617, 1622, 1625, 1626, 1629, 1630, 1631, 1636, 1638, 1639,\n",
       "       1640, 1649, 1651, 1654, 1658, 1664, 1666, 1668, 1671, 1685, 1689,\n",
       "       1693, 1696, 1705, 1709, 1710, 1711, 1724, 1725, 1730, 1735, 1736,\n",
       "       1750, 1761, 1762, 1764, 1766, 1767, 1780, 1789, 1790, 1802, 1805,\n",
       "       1806, 1807, 1808, 1811, 1823, 1825, 1828, 1834, 1841, 1847, 1849,\n",
       "       1857, 1869, 1882, 1883, 1889, 1905, 1913, 1915, 1917, 1918, 1919,\n",
       "       1920, 1923, 1925, 1936, 1941, 1949, 1952, 1956, 1959, 1962, 1963,\n",
       "       1969, 1977, 1983, 1993, 1994, 2000, 2005, 2026, 2029, 2030, 2043,\n",
       "       2058, 2061, 2064, 2069, 2070, 2071, 2072, 2075, 2076, 2078, 2079,\n",
       "       2080, 2096, 2100, 2101, 2125, 2147, 2153, 2154, 2155, 2165, 2183,\n",
       "       2185, 2187, 2191, 2200, 2206, 2219, 2221, 2225, 2226, 2227, 2233,\n",
       "       2241, 2242, 2246, 2250, 2251, 2252, 2256, 2264, 2271, 2276, 2282,\n",
       "       2284, 2286, 2288, 2291, 2297, 2304, 2312, 2319, 2322, 2333, 2339,\n",
       "       2346, 2349, 2352, 2358, 2368, 2370, 2376, 2377, 2387, 2389, 2394,\n",
       "       2395, 2401, 2403, 2411, 2412, 2415, 2416, 2417, 2420, 2424, 2425,\n",
       "       2426, 2442, 2443, 2445, 2450, 2456, 2461, 2470, 2478, 2487, 2488,\n",
       "       2500, 2501, 2507, 2509, 2529, 2537, 2539, 2540, 2541, 2556, 2563,\n",
       "       2565, 2571, 2577, 2578, 2581, 2582, 2584, 2588, 2591, 2593, 2616,\n",
       "       2617, 2620, 2621, 2625, 2656, 2668, 2677, 2681, 2684, 2694, 2695,\n",
       "       2698, 2699, 2703, 2707, 2709, 2710, 2715, 2717, 2718, 2729, 2731,\n",
       "       2746, 2750, 2751, 2757, 2762, 2774, 2775, 2779, 2790, 2798, 2801,\n",
       "       2811, 2813, 2827, 2832, 2836, 2839, 2845, 2850, 2851, 2853, 2855,\n",
       "       2856, 2863, 2864, 2866, 2868, 2870, 2873, 2874, 2877, 2885, 2897,\n",
       "       2906, 2911, 2912, 2916, 2923, 2939, 2948, 2950, 2951, 2954, 2967,\n",
       "       2981, 2990, 2994, 2995, 2996, 2999, 3010, 3012, 3014, 3019, 3031,\n",
       "       3040, 3042, 3044, 3045, 3047, 3053, 3059, 3066, 3071, 3077, 3082,\n",
       "       3087, 3094, 3103, 3113, 3120, 3123, 3144, 3152, 3153, 3160, 3163,\n",
       "       3166, 3172, 3181, 3183, 3184, 3199, 3209, 3210, 3215, 3218, 3219,\n",
       "       3222, 3227, 3241, 3247, 3249, 3250, 3255, 3258, 3262, 3263, 3264,\n",
       "       3265, 3271, 3278, 3285, 3298, 3305, 3314, 3319, 3321, 3324, 3332,\n",
       "       3336, 3338, 3345, 3348, 3349, 3357, 3359, 3364, 3384, 3385, 3392,\n",
       "       3396, 3400, 3402, 3415, 3418, 3424, 3426, 3427, 3430, 3431, 3432,\n",
       "       3433, 3447, 3451, 3453, 3455, 3462, 3465, 3469, 3470, 3475, 3501,\n",
       "       3517, 3518, 3519, 3533, 3541, 3542, 3545, 3546, 3553, 3555, 3556,\n",
       "       3561, 3564, 3567, 3580, 3582, 3583, 3590, 3592, 3595, 3598, 3601,\n",
       "       3603, 3609, 3625, 3629, 3631, 3633, 3634, 3637, 3638, 3639, 3641,\n",
       "       3643, 3644, 3652, 3653, 3658, 3659, 3661, 3663, 3664, 3666, 3670,\n",
       "       3673, 3682, 3684, 3704, 3706, 3709, 3714, 3716, 3718, 3724, 3729,\n",
       "       3731, 3735, 3746, 3747, 3755, 3766, 3771, 3776, 3781, 3785, 3787,\n",
       "       3793, 3795, 3798, 3800, 3808, 3822, 3826, 3829, 3834, 3838, 3844,\n",
       "       3851, 3852, 3862, 3866, 3871, 3872, 3886, 3891, 3893, 3899, 3903,\n",
       "       3907, 3921, 3934, 3936, 3939, 3940, 3941, 3944, 3946, 3951, 3959,\n",
       "       3962, 3968, 3971, 3975, 3989, 3990, 3994, 3997])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
